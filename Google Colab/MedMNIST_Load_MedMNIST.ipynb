{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepakbastola/AI_Science_Medicine/blob/main/Google%20Colab/MedMNIST_Load_MedMNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision.transforms.functional import normalize\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "sLoy_mhqKxAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Select GPU if available"
      ],
      "metadata": {
        "id": "lnt95HMhljJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scale_img_255(img):\n",
        "    return ((img - img.min()) / (img.max() - img.min()) * 255).astype(np.uint8)     # Make sure it's uint8 and in the range [0, 255]\n",
        "\n",
        "def imshow(img):\n",
        "    img = img.cpu().numpy()    \n",
        "    img = scale_img_255(img)\n",
        "    plt.imshow(np.transpose(img, (1, 2, 0)))\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "2I9i5fOIKxA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def GPU(data):\n",
        "    return torch.tensor(data, requires_grad=True, dtype=torch.float, device=device)\n",
        "def GPU_data(data):\n",
        "    return torch.tensor(data, requires_grad=False, dtype=torch.float, device=device)"
      ],
      "metadata": {
        "id": "KnbGyXTlL5YR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset URLS\n",
        "# https://zenodo.org/record/6496656/files/pathmnist.npz\n",
        "# https://zenodo.org/record/6496656/files/bloodmnist.npz\n",
        "# https://zenodo.org/record/6496656/files/chestmnist.npz\n",
        "# https://zenodo.org/record/6496656/files/dermamnist.npz\n",
        "# https://zenodo.org/record/6496656/files/breastmnist.npz\n",
        "# https://zenodo.org/record/6496656/files/octmnist.npz\n",
        "# https://zenodo.org/record/6496656/files/organamnist.npz\n",
        "# https://zenodo.org/record/6496656/files/organcmnist.npz\n",
        "# https://zenodo.org/record/6496656/files/organsmnist.npz\n",
        "# https://zenodo.org/record/6496656/files/pathmnist.npz\n",
        "# https://zenodo.org/record/6496656/files/pneumoniamnist.npz\n",
        "# https://zenodo.org/record/6496656/files/tissuemnist.npz\n",
        "# https://zenodo.org/record/6496656/files/vesselmnist3d.npz\n",
        "# https://zenodo.org/record/6496656/files/synapsemnist3d.npz\n",
        "# https://zenodo.org/record/6496656/files/adrenalmnist3d.npz\n",
        "# https://zenodo.org/record/6496656/files/fracturemnist3d.npz\n",
        "# https://zenodo.org/record/6496656/files/nodulemnist3d.npz\n",
        "# https://zenodo.org/record/6496656/files/organmnist3d.npz"
      ],
      "metadata": {
        "id": "U5wmD_lNK0wR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://zenodo.org/record/6496656/files/pathmnist.npz"
      ],
      "metadata": {
        "id": "TRTXGZlqKxAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.load('pathmnist.npz')"
      ],
      "metadata": {
        "id": "yX6VmQXBKxAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ebf3777-1599-466a-b92a-634148b7612d",
        "id": "s1bZ-hgGKxAy"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['train_images', 'val_images', 'test_images', 'train_labels', 'val_labels', 'test_labels']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = data['train_images']\n",
        "y = data['train_labels']\n",
        "\n",
        "x_test = data['test_images']\n",
        "y_test = data['test_labels']\n",
        "\n",
        "y = y[:,0]\n",
        "y_test = y_test[:,0]\n",
        "\n",
        "x = GPU_data(x)\n",
        "y = GPU_data(y).to(torch.int64)\n",
        "\n",
        "x_test = GPU_data(x_test)\n",
        "y_test = GPU_data(y_test).to(torch.int64)"
      ],
      "metadata": {
        "id": "nJXtLYYEKxAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scale_img(img):\n",
        "    img_min = torch.min(img)\n",
        "    img_max = torch.max(img)\n",
        "    scaled_img = (img - img_min) / (img_max - img_min) * 1.0\n",
        "    return scaled_img"
      ],
      "metadata": {
        "id": "lHTrzVNa9LUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = scale_img(x)\n",
        "x_test = scale_img(x_test)"
      ],
      "metadata": {
        "id": "FNQNNgYc9MJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot(x):\n",
        "    if type(x) == torch.Tensor :\n",
        "        x = x.cpu().detach().numpy()\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(x, cmap = 'gray')\n",
        "    ax.axis('off')\n",
        "    fig.set_size_inches(10, 10)\n",
        "    plt.show()\n",
        "\n",
        "def montage(images_tensor):\n",
        "    b, c, h, w = images_tensor.shape # batch size, # of channels, height & weight of tensor\n",
        "\n",
        "    if w == 3:\n",
        "        images_tensor = np.transpose(images_tensor,(0,3,1,2))\n",
        "        b, c, h, w = images_tensor.shape\n",
        "\n",
        "    assert c == 1 or c == 3, \"Number of channels must be either 1 (grayscale) or 3 (RGB)\"\n",
        "    \n",
        "    montage_w = int(np.ceil(np.sqrt(b))) # ensures the montage can accommodate all images in the batch while maintaining a square shape\n",
        "    montage_h = int(np.ceil(b / montage_w)) # ensures that the montage image has enough rows to accommodate all the images in the batch\n",
        "    \n",
        "    if c == 1:\n",
        "        montage_image = np.zeros((montage_h * h, montage_w * w)) # create blank montage w the same number of channels as the input images (1, gray).\n",
        "    else:\n",
        "        montage_image = np.zeros((montage_h * h, montage_w * w, c)) # creates color montage\n",
        "    \n",
        "    for idx, img in enumerate(images_tensor): # correctly position each image in the montage grid based on its index\n",
        "        i = idx // montage_w\n",
        "        j = idx % montage_w\n",
        "        if c == 1: # assigns each image in the tensor to the corresponding region in the montage based on its row and column indices\n",
        "            montage_image[i * h:(i + 1) * h, j * w:(j + 1) * w] = img.squeeze(0)\n",
        "        else:\n",
        "            montage_image[i * h:(i + 1) * h, j * w:(j + 1) * w, :] = np.transpose(img, (1, 2, 0))\n",
        "        \n",
        "    return montage_image"
      ],
      "metadata": {
        "id": "rW4L3t8CgGlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KC8Vlku9gxjG",
        "outputId": "562ae653-8c9a-4836-fc5f-0f72e19f1155"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([89996, 28, 28, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot(montage(x[:100]))"
      ],
      "metadata": {
        "id": "56RvDSulhTUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y2NUrqgAhXvq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}